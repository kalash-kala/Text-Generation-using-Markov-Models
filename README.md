# Text Generation using Markov Model

### Problems with Markov assumption :

- In this assumption, we assume that the current word depends on the previous word, which is not true in real life
- So to counter this we consider that the current word depends on more than 1 of the previous words (N-Gram Approach)
- In this project we have gone to $2^{nd}$ order Markov Assumption (current word depends on 2 previous words)
    
    ![Screenshot 2023-04-18 at 11.38.36 PM.png](Text%20Generation%20using%20Markov%20Model%20Screenshots/Screenshot_2023-04-18_at_11.38.36_PM.png)
    

### Initialisations:

- Created 3 dictionaries to store the following :
    - Initial State Distribution (probability of a word to be the first word in the sentence)
        
        ![Screenshot 2023-04-18 at 11.39.01 PM.png](Text%20Generation%20using%20Markov%20Model%20Screenshots/Screenshot_2023-04-18_at_11.39.01_PM.png)
        
    - First order Markov Distribution (probability of a word to occur after the previous word)
        
        ![Screenshot 2023-04-18 at 11.39.21 PM.png](Text%20Generation%20using%20Markov%20Model%20Screenshots/Screenshot_2023-04-18_at_11.39.21_PM.png)
        
    - Second order Markov Distribution (probability of a word to occur after the previous 2 words)
        
        ![Screenshot 2023-04-18 at 11.39.56 PM.png](Text%20Generation%20using%20Markov%20Model%20Screenshots/Screenshot_2023-04-18_at_11.39.56_PM.png)
        

### Steps to fill the dictionaries :

1. Initial State Distribution :
    1. Store frequency of each initial word of the sentence, in the dictionary
    2. Divide all the values by total number of sentences in the corpus
2. First order Markov Distribution :
    1. This dictionary is a nested dictionary
    2. Outer keys are the previous word and inner keys are the words coming after them in the sentence, we store this frequency in the dictionary for keys (previous_word, current_word)
    3. Divide the frequency by the total frequency of words for the previous word or outer key
3. Second order Markov Distribution : 
    1. Similar to first order Markov Distribution, only difference is that there will be 2 outer keys

### Why use Dictionaries instead of Multidimensional numpy arrays or lists :

- Dictionaries help us overcome the **************************************************CURSE OF DIMENSIONALITY :**************************************************
- Curse of Dimensionality basically states that as the number of dimensions increase, the amount of memory consumed to store the data increases exponentially and most of the volume of space is empty. Most of the data tends to occupy the surface area of such storage space with n dimensions, leaving the 90-95  % of the interior volume empty.
- Dictionaries avoid this space wastage by storing different amounts of data for each word, some words can have n words as options after them , some can have only 1, so it gives us this flexibility
- Basically Dictionaries help us save space

### How to handle Key error for first order and second order Markov dictionary :

- There might be a case where the word generated by these dictionaries may not be present as outer or inner key resulting in key error for second order Markov dictionary
- So to handle this, we have added a case that when the keys are absent in the dictionary , fetch a word from the initial state distribution.
- The problem with this approach is that the sentence will start to not make sense, this can be considered as a tradeoff
- If you have better ideas to handle this then do raise an issue